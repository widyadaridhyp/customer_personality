# -*- coding: utf-8 -*-
"""research.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O73AKyyX61VyfCouGZZE71COgVQ0YhVE

# Load Data
"""

import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv(r'../data/marketing_campaign.csv', sep='\t')

df.info()

df.head()

df.columns = df.columns.str.lower()

df.response.value_counts()

"""imbalance"""

df.drop(columns=[
    'id','year_birth', 'dt_customer',
    'z_costcontact','z_revenue'
], inplace=True)

df.to_csv(f'../data/data_raw.csv')

"""# EDA"""

df_all = pd.read_csv(f'../data/data_raw.csv', index_col=0)

df_all.columns

df_all.duplicated().sum()

df_all['frequency'] = (
    df_all['numdealspurchases'] +
    df_all['numwebpurchases'] +
    df_all['numcatalogpurchases'] +
    df_all['numstorepurchases']
)

df_all['monetary'] = (
    df_all['mntwines'] +
    df_all['mntfruits'] +
    df_all['mntmeatproducts'] +
    df_all['mntfishproducts'] +
    df_all['mntsweetproducts'] +
    df_all['mntgoldprods']
)

df_all.isnull().sum()

df_all.dropna(inplace=True)

df_new = df_all.copy()

df_new.drop(columns=[
    'mntwines', 'mntfruits',
    'mntmeatproducts', 'mntfishproducts', 'mntsweetproducts',
    'mntgoldprods', 'numdealspurchases', 'numwebpurchases',
    'numcatalogpurchases', 'numstorepurchases'
], inplace=True)

df_new.info()

num = df_new.select_dtypes(include='number')
cat = df_new.select_dtypes(exclude='number')

"""## Numeric"""

plt.figure(figsize=(10,5))
sns.heatmap(num.corr(), annot=True)

cat = [
    'education',
    'marital_status',
    'acceptedcmp1',
    'acceptedcmp2',
    'acceptedcmp3',
    'acceptedcmp4',
    'acceptedcmp5',
    'complain'
]

num = df_new.columns.drop(
    cat
)

target = 'response'
num = num.drop('response')

sns.heatmap(df_new[num].corr(), annot=True)

from statsmodels.stats.outliers_influence import variance_inflation_factor

X = df_new[num].copy()
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif_data

"""$\therefore$ multikolineritas untuk frequency, sehingga dihilangkan"""

num = num.drop('frequency')

X = df_new[num].copy()
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif_data

sns.heatmap(df_new[num].corr(), annot=True)

sns.pairplot(df_new[num])

frequency = ['numdealspurchases', 'numwebpurchases',
            'numcatalogpurchases', 'numstorepurchases']

cols = list(num) + list(frequency)

cols

df_new.columns

X = df_all[cols].copy()
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif_data

sns.heatmap(df_all[cols].corr(), annot=True)

plt.figure(figsize=(20, 15))

for i, col in enumerate(cols, 1):
    plt.subplot(3, 4, i)
    sns.histplot(df_all[col], kde=True, bins=30, color='teal')
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel("Count")

plt.tight_layout()
plt.show()

"""- Akan dilakukan scaling MinMaxScaler:
    - numwebpurchases
    - numstorepurchases
    - monetary
    - numdealspurchases
    - numcatalogpurchases
    - income
    - recency
    - numwebvisitsmonth
"""

plt.figure(figsize=(20, 15))

for i, col in enumerate(cols, 1):
    plt.subplot(3, 4, i)
    sns.boxplot(df_all[col], color='teal')
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel("Count")

plt.tight_layout()
plt.show()

"""## Category"""

df_new['education'].value_counts()

df_new['marital_status'].value_counts()

df_new.columns

"""### Encode"""

df_encoded = df_new[cat].copy()

df_encoded['education'].replace({
    '2n Cycle': 'Master'
}, inplace=True)

edu_map ={
    'Basic': 0,
    'Graduation': 1,
    'Master': 2,
    'PhD': 3
}

df_encoded["education_ord"] = df_encoded["education"].map(edu_map)

df_encoded['marital_status'] = df_encoded['marital_status'].replace({
    'Widow':'Divorced',
    'Alone': 'Single',
    'YOLO': 'Single',
    'Absurd': 'Single'
})

marital_encode_map = {
    'Married': 0,
    'Together': 1,
    'Divorced': 2,
    'Single': 3
}

df_encoded["marital_status_ord"] = df_encoded["marital_status"].map(marital_encode_map)

df_encoded.drop(columns=
                ['marital_status','education'],
                inplace=True)

"""### Chi-Square"""

from scipy.stats import chi2_contingency

results = []

for var in cat:
    contingency_table = pd.crosstab(df_new[var], df_new['response'])
    chi2, p, dof, expected = chi2_contingency(contingency_table)

    results.append({
        'Variable': var,
        'Chi-square': chi2,
        'p-value': p,
        'dof': dof
    })

chi_results = pd.DataFrame(results)
chi_results

"""$\therefore$ Karena tidak signifikan, complain akan didrop"""

df_encoded.drop(columns = 'complain', inplace=True)

fix = pd.concat([df_encoded, df_all[cols], df_all[target]], axis=1)
fix.head()

fix.response.value_counts()

fix.columns

"""## Why?"""

plt.figure(figsize=(18, 5))

campaign_cols = ["acceptedcmp1","acceptedcmp2","acceptedcmp3","acceptedcmp4","acceptedcmp5"]

for i, col in enumerate(campaign_cols, 1):
    plt.subplot(1, 5, i)
    sns.countplot(data=fix, x=col, hue="response")
    plt.title(f"{col} vs Response")
    plt.xlabel(col)
    plt.ylabel("Count")

plt.tight_layout()
plt.show()

"""- Mayoritas customer belum pernah menerima campaign sebelumnya (accepted=0)
- Tapi dari yang pernah menerima campaign sebelumnya (accepted=1), proporsi yang merespons jauh lebih tinggi
"""

sns.boxplot(data=fix, x="response", y="income", palette='crest')
plt.title("Income vs Response")
plt.xlabel("Response")
plt.ylabel("Income")
plt.show()

"""Customer dengan pendapatan lebih tinggi lebih responsif terhadap campaign marketing terbaru."""

sns.boxplot(data=fix, x="response", y="monetary", palette='crest')
plt.title("Distribution of Monetary by Response")
plt.xlabel("Monetary")
plt.ylabel("Count")
plt.show()

"""- Customer dengan purchasing power tinggi adalah yang paling responsif.
- Mereka adalah pelanggan valuable yang aktif melakukan transaksi.
"""

edu_ct = pd.crosstab(fix['education_ord'], fix['response'])

# calculate % per response
edu_pct = edu_ct.div(edu_ct.sum(axis=0), axis=1) * 100

# plot stacked bar where X = response
ax = edu_ct.T.plot(
    kind='bar',
    stacked=True,
    figsize=(10,6),
    colormap='crest'
)

plt.title("Stacked Count of Education by Response (with %)")
plt.xlabel("Response")
plt.ylabel("Count")
plt.legend(title='Education Ord')

# Add % labels on each stack
for response_idx, response_val in enumerate(edu_ct.columns):
    cumulative = 0
    for edu_idx in range(len(edu_ct)):
        val = edu_ct.iloc[edu_idx, response_idx]
        pct = edu_pct.iloc[edu_idx, response_idx]
        if val > 0:
            ax.text(
                response_idx,
                cumulative + val/2,
                f"{pct:.1f}%",
                ha='center', va='center', fontsize=9
            )
        cumulative += val

plt.tight_layout()
plt.show()

"""- Response tertinggi datang dari kelompok pendidikan yang lebih tinggi (ordinal 2 dan 3).
- Kelompok pendidikan rendah hampir tidak merespons campaign bisa jadi ini termasuk faktor income juga.
- Pelanggan berpendidikan tinggi:
    - Lebih paham nilai promo
    - Mungkin lebih engaged dengan brand
    - Lebih sering membaca pesan marketing
"""

mar_ct = pd.crosstab(fix['marital_status_ord'], fix['response'])

# calculate % per response group
mar_pct = mar_ct.div(mar_ct.sum(axis=0), axis=1) * 100

# plot stacked bar (X = response)
ax = mar_ct.T.plot(
    kind='bar',
    stacked=True,
    figsize=(10,6),
    colormap='crest'
)

plt.title("Stacked Count of Marital Status by Response (with %)")
plt.xlabel("Response")
plt.ylabel("Count")
plt.legend(title='Marital Status Ord')

# Add % labels inside each stack
for response_idx, response_val in enumerate(mar_ct.columns):
    cumulative = 0
    for mar_idx in range(len(mar_ct)):
        val = mar_ct.iloc[mar_idx, response_idx]
        pct = mar_pct.iloc[mar_idx, response_idx]
        if val > 0:
            ax.text(
                response_idx,
                cumulative + val/2,
                f"{pct:.1f}%",
                ha='center', va='center', fontsize=9
            )
        cumulative += val

plt.tight_layout()
plt.show()

plt.figure(figsize=(7,5))
sns.scatterplot(data=fix, x="recency", y="monetary", hue="response", palette="coolwarm")
plt.title("Recency vs Monetary by Response")
plt.xlabel("Recency")
plt.ylabel("Monetary")
plt.show()

"""- Tidak ada pola jelas antara recency dan monetary secara langsung.
- Tetapi:
    - Customer dengan monetary tinggi dan recency rendah â†’ lebih banyak yang merespons.
    - Banyak titik response=1 berada di area spending tinggi.
"""

sns.boxplot(data=fix, x="response", y="numwebvisitsmonth", palette='crest')
plt.title("Number Web Visit vs Response")
plt.xlabel("Response")
plt.ylabel("Number Web Visit")
plt.show()

"""- Semakin sering pelanggan mengunjungi website, semakin besar kemungkinan mereka merespons campaign.
- Mengindikasikan adanya digital engagement effect.

# Modeling

## Feature Scaling
"""

from sklearn.preprocessing import MinMaxScaler

target = df_all['response']
features = fix.drop(columns='response')

from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)
X_train_res, y_train_res = rus.fit_resample(X_train, y_train)

def scaled(X_train, X_test):
    scaler = MinMaxScaler()

    cols = [
        "income",
        "recency",
        "numwebvisitsmonth",
        "numwebpurchases",
        "numstorepurchases",
        "monetary",
        "numdealspurchases",
        "numcatalogpurchases"
    ]

    # =======================
    # Fit scaler on TRAIN ONLY
    # =======================
    X_train_scaled = pd.DataFrame(
        scaler.fit_transform(X_train[cols]),
        columns=[c + "_scaled" for c in cols],
        index=X_train.index
    )

    # Transform TEST (no fitting)
    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test[cols]),
        columns=[c + "_scaled" for c in cols],
        index=X_test.index
    )

    # =======================
    # Combine back with other features
    # =======================
    others_train = X_train.drop(columns=cols)
    X_train_final = pd.concat([others_train, X_train_scaled], axis=1)

    others_test = X_test.drop(columns=cols)
    X_test_final = pd.concat([others_test, X_test_scaled], axis=1)

    return X_train_final, X_test_final

X_train_scaled, X_test_scaled = scaled(X_train_res, X_test)

"""## Feature Decomposition"""

from sklearn.decomposition import PCA

def automatic_pca(X_train, X_test, variance_threshold=0.95):
    print("Fitting PCA...")

    # Fit PCA tanpa n_components dulu (full PCA)
    pca_full = PCA()
    pca_full.fit(X_train)

    explained = pca_full.explained_variance_ratio_
    cumulative = np.cumsum(explained)

    # Tentukan jumlah komponen otomatis
    n_components_optimal = np.argmax(cumulative >= variance_threshold) + 1

    print(f"Variance threshold   : {variance_threshold*100:.1f}%")
    print(f"Total components    : {X_train.shape[1]}")
    print(f"Optimal components  : {n_components_optimal}")
    print(f"Explained variance  : {cumulative[n_components_optimal-1]*100:.2f}%")

    # Fit PCA ulang dengan komponen optimal
    pca = PCA(n_components=n_components_optimal)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)

    # Scree plot
    plt.figure(figsize=(8,5))
    plt.plot(range(1, len(explained)+1), explained, marker='o', label='Individual Variance')
    plt.plot(range(1, len(cumulative)+1), cumulative, marker='x', label='Cumulative Variance')
    plt.axvline(n_components_optimal, color='red', linestyle='--', label='Optimal Components')
    plt.title("Automatic PCA: Scree Plot")
    plt.xlabel("Principal Component")
    plt.ylabel("Variance Ratio")
    plt.legend()
    plt.grid()
    plt.show()

    return X_train_pca, X_test_pca, n_components_optimal

X_train_pca_scaled, X_test_pca_scaled, n_comp_scaled = automatic_pca(X_train_scaled, X_test_scaled)

"""## Modelling (- Hyperparameter)"""

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

positive = sum(y_train_res == 1)
negative = sum(y_train_res == 0)
models = {
    "Logistic Regression": LogisticRegression(class_weight='balanced'),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(class_weight='balanced'),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "XGBoost": XGBClassifier(
        random_state=42,
        eval_metric="logloss",
        use_label_encoder=False,
        scale_pos_weight = negative/positive
    )
}

from sklearn.metrics import classification_report, accuracy_score

results = []

for name, model in models.items():
    model.fit(X_train_pca_scaled, y_train_res)
    y_pred = model.predict(X_test_pca_scaled)

    # ambil classification report sebagai dict
    report = classification_report(y_test, y_pred, output_dict=True)

    # ambil nilai recall, precision, f1 untuk kelas 1 (positive class)
    precision = report["1"]["precision"]
    recall = report["1"]["recall"]
    f1 = report["1"]["f1-score"]

    # accuracy
    accuracy = accuracy_score(y_test, y_pred)

    results.append([name, accuracy, precision, recall, f1])

results_df = pd.DataFrame(
    results,
    columns=["Model", "Accuracy", "Precision", "Recall", "F1"]
)

results_df

"""# Modeling (+Hyperparameter)"""

import optuna
from optuna.samplers import TPESampler

def objective(trial):

    model_name = trial.suggest_categorical(
        "model",
        ["logit", "knn", "dt", "rf", "gb", "xgb"]
    )

    # LOGISTIC REGRESSION
    if model_name == "logit":
        C = trial.suggest_float("C", 0.001, 10, log=True)
        model = LogisticRegression(
            C=C,
            class_weight='balanced',
            max_iter=500
        )

    # KNN
    elif model_name == "knn":
        n = trial.suggest_int("n_neighbors", 3, 25)
        model = KNeighborsClassifier(n_neighbors=n)

    # DECISION TREE
    elif model_name == "dt":
        max_depth = trial.suggest_int("max_depth", 2, 20)
        model = DecisionTreeClassifier(
            max_depth=max_depth,
            random_state=42
        )

    # RANDOM FOREST
    elif model_name == "rf":
        n_estimators = trial.suggest_int("n_estimators", 100, 800)
        max_depth = trial.suggest_int("max_depth", 2, 20)
        model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            class_weight="balanced",
            random_state=42
        )

    # GRADIENT BOOSTING
    elif model_name == "gb":
        n_estimators = trial.suggest_int("n_estimators", 100, 800)
        lr = trial.suggest_float("learning_rate", 0.01, 0.3)
        model = GradientBoostingClassifier(
            n_estimators=n_estimators,
            learning_rate=lr,
            random_state=42
        )

    # XGBOOST
    elif model_name == "xgb":
        max_depth = trial.suggest_int("max_depth", 3, 10)
        lr = trial.suggest_float("learning_rate", 0.01, 0.3)
        n_estimators = trial.suggest_int("n_estimators", 100, 800)
        subsample = trial.suggest_float("subsample", 0.5, 1.0)

        model = XGBClassifier(
            max_depth=max_depth,
            learning_rate=lr,
            n_estimators=n_estimators,
            subsample=subsample,
            eval_metric="logloss",
            scale_pos_weight=negative/positive,
            random_state=42,
            use_label_encoder=False
        )

    # FIT MODEL
    model.fit(X_train_pca_scaled, y_train_res)
    y_pred = model.predict(X_test_pca_scaled)

    report = classification_report(y_test, y_pred, output_dict=True)
    precision = report["1"]["precision"]
    recall = report["1"]["recall"]
    f1 = report["1"]["f1-score"]
    accuracy = accuracy_score(y_test, y_pred)

    return recall

study = optuna.create_study(
    direction="maximize",
    sampler=TPESampler(seed=42)
)

study.optimize(objective, n_trials=50)

print("Best Recall:", study.best_value)
print("Best Parameters:", study.best_params)

best = study.best_params

model_map = {}

if best["model"] == "logit":
    model = LogisticRegression(
        C=best["C"],
        class_weight='balanced',
        max_iter=500
    )

elif best["model"] == "knn":
    model = KNeighborsClassifier(
        n_neighbors=best["n_neighbors"]
    )

elif best["model"] == "dt":
    model = DecisionTreeClassifier(
        max_depth=best["max_depth"],
        random_state=42
    )

elif best["model"] == "rf":
    model = RandomForestClassifier(
        n_estimators=best["n_estimators"],
        max_depth=best["max_depth"],
        class_weight="balanced",
        random_state=42
    )

elif best["model"] == "gb":
    model = GradientBoostingClassifier(
        n_estimators=best["n_estimators"],
        learning_rate=best["learning_rate"],
        random_state=42
    )

elif best["model"] == "xgb":
    model = XGBClassifier(
        max_depth=best["max_depth"],
        learning_rate=best["learning_rate"],
        n_estimators=best["n_estimators"],
        subsample=best["subsample"],
        scale_pos_weight=negative/positive,
        eval_metric="logloss",
        random_state=42,
        use_label_encoder=False
    )

# Final fit
model.fit(X_train_pca_scaled, y_train_res)
y_pred_final = model.predict(X_test_pca_scaled)